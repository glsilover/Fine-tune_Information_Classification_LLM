{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76de9fce-bb68-45c7-8947-573ee32f6a6a",
   "metadata": {},
   "source": [
    "# Part 1 : 数据爬取\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b408e84-329d-46be-be9e-5b9e4b86840b",
   "metadata": {},
   "source": [
    "### 任务目标\n",
    "1. 利用feedparser库爬取RSS类信息流；\n",
    "2. 利用BeautifulSoup库爬取通用HTML网页元素；\n",
    "3. 利用selenium模拟真实用户行为爬取特殊的网页内容；\n",
    "4. 将爬取的内容插入金山云RDS。\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "efbedd62-5d23-47b3-8548-f3f51ede98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from IPython.display import display\n",
    "import mysql.connector\n",
    "import json\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.tz import gettz\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fad1e4-e2eb-4b74-a306-398f165e3e58",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "## 一、环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "01c10b93-77e9-4298-9ee4-76c655d20b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'config_new.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "63a63694-cbca-48be-b2b3-00577e9331e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义时间转换函数\n",
    "\n",
    "china_timezone = timezone(timedelta(hours=8))\n",
    "def china_time_converter(*args):\n",
    "    \"\"\"\n",
    "    自定义时间转换器，将UTC时间转换为北京时间。\n",
    "    \"\"\"\n",
    "    utc_dt = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    return utc_dt.astimezone(china_timezone).timetuple()\n",
    "\n",
    "def convert_to_cst(datetime_str):\n",
    "    # 解析日期字符串\n",
    "    try:\n",
    "        return parser.parse(datetime_str).astimezone(gettz(\"Asia/Shanghai\"))\n",
    "    except Exception as e:\n",
    "        return datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "20948b04-1cf7-4291-936b-f5baddd1b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_source_list = []\n",
    "web_source_list = []\n",
    "\n",
    "rss_source_list = [config[source] for source in config['Source_List'] if config[source]['type'] == 'rss']\n",
    "web_source_list = [config[source] for source in config['Source_List'] if config[source]['type'] != 'rss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0145967e-2276-4733-aed5-22169700ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://aws.amazon.com/cn/blogs/machine-learni...</td>\n",
       "      <td>rss</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://research.fb.com/feed/</td>\n",
       "      <td>rss</td>\n",
       "      <td>Meta-Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://deepmind.google/blog/rss.xml</td>\n",
       "      <td>rss</td>\n",
       "      <td>DeepMind-Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://blogs.microsoft.com/ai/feed/rss</td>\n",
       "      <td>rss</td>\n",
       "      <td>Microsoft_Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.infoq.cn/feed/</td>\n",
       "      <td>rss</td>\n",
       "      <td>InfoQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.36kr.com/feed-article</td>\n",
       "      <td>rss</td>\n",
       "      <td>36kr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://sspai.com/feed</td>\n",
       "      <td>rss</td>\n",
       "      <td>sspai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url type               folder\n",
       "0  https://aws.amazon.com/cn/blogs/machine-learni...  rss  AWS-MachineLearning\n",
       "1                      https://research.fb.com/feed/  rss        Meta-Research\n",
       "2               https://deepmind.google/blog/rss.xml  rss      DeepMind-Google\n",
       "3            https://blogs.microsoft.com/ai/feed/rss  rss       Microsoft_Blog\n",
       "4                         https://www.infoq.cn/feed/  rss                InfoQ\n",
       "5                  https://www.36kr.com/feed-article  rss                 36kr\n",
       "6                             https://sspai.com/feed  rss                sspai"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "      <th>folder</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openai.com/news/?limit=10</td>\n",
       "      <td>web</td>\n",
       "      <td>OpenAI_News</td>\n",
       "      <td>https://openai.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.deeplearning.ai/the-batch/</td>\n",
       "      <td>web</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "      <td>https://www.deeplearning.ai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      url type           folder  \\\n",
       "0       https://openai.com/news/?limit=10  web      OpenAI_News   \n",
       "1  https://www.deeplearning.ai/the-batch/  web  deeplearning_ai   \n",
       "\n",
       "                        domain  \n",
       "0           https://openai.com  \n",
       "1  https://www.deeplearning.ai  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(rss_source_list))\n",
    "display(pd.DataFrame(web_source_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c7de9-ef96-4262-90ea-9feb4631ae1b",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "## 二、内容爬取\n",
    "\n",
    "### 2.1 通过feedparser获取RSS源数据\n",
    "\n",
    "feedparser 是一个用来解析 RSS 和 Atom feeds 的 Python 库，用于从网络抓取和处理内容聚合数据，例如新闻、博客文章、播客等。只能解析标准化的 RSS 或 Atom feed 数据。能快速提取 feed 标题、条目标题、链接、摘要等。\n",
    "![feedparser信息流展示](pictures/feedparser信息流展示.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "537ac045-8a78-40d0-ad43-9fb43d5d3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rss_article(source_content):\n",
    "    content_list = []\n",
    "    feed = feedparser.parse(source_content.get('url'))\n",
    "    print(f\"解析成功: {source_content.get('url')}, 共找到{len(feed.entries)}篇文章。\\n\")\n",
    "    for entry in feed.entries:\n",
    "        try:\n",
    "            content_dict = {\n",
    "                'title': entry.get('title', '无标题'),\n",
    "                'link': entry.get('link', '未知链接'),\n",
    "                'description_original': BeautifulSoup(entry.get('summary', '无描述'), \"lxml\").get_text(),  #由于summary并非纯文本格式，需要使用bs来去除HTML标签\n",
    "                'source': source_content.get('folder'),\n",
    "                'published_at': convert_to_cst(entry.get('published', '未知时间')),\n",
    "                'folder': source_content.get('folder')\n",
    "            }\n",
    "            content_list.append(content_dict)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    print('本次解析任务完成\\n---')\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "89039408-2c6f-44d6-b41e-db4944e15d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解析成功: https://aws.amazon.com/cn/blogs/machine-learning/feed/, 共找到20篇文章。\n",
      "\n",
      "本次解析任务完成\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "content_list = get_rss_article(rss_source_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9a045dc6-1a33-4fcf-8996-da6656f70e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>description_original</th>\n",
       "      <th>source</th>\n",
       "      <th>published_at</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How Amazon trains sequential ensemble models a...</td>\n",
       "      <td>https://aws.amazon.com/blogs/machine-learning/...</td>\n",
       "      <td>Ensemble models are becoming popular within th...</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "      <td>2024-12-16 17:28:15.600928</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Implementing login node load balancing in Sage...</td>\n",
       "      <td>https://aws.amazon.com/blogs/machine-learning/...</td>\n",
       "      <td>In this post, we explore a solution for implem...</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "      <td>2024-12-16 17:28:15.601269</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Clearwater Analytics is revolutionizing in...</td>\n",
       "      <td>https://aws.amazon.com/blogs/machine-learning/...</td>\n",
       "      <td>In this post, we explore Clearwater Analytics’...</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "      <td>2024-12-16 17:28:15.601499</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Twitch used agentic workflow with RAG on A...</td>\n",
       "      <td>https://aws.amazon.com/blogs/machine-learning/...</td>\n",
       "      <td>In this post, we demonstrate how we innovated ...</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "      <td>2024-12-16 17:28:15.601714</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accelerate analysis and discovery of cancer bi...</td>\n",
       "      <td>https://aws.amazon.com/blogs/machine-learning/...</td>\n",
       "      <td>Bedrock multi-agent collaboration enables deve...</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "      <td>2024-12-16 17:28:15.601947</td>\n",
       "      <td>AWS-MachineLearning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  How Amazon trains sequential ensemble models a...   \n",
       "1  Implementing login node load balancing in Sage...   \n",
       "2  How Clearwater Analytics is revolutionizing in...   \n",
       "3  How Twitch used agentic workflow with RAG on A...   \n",
       "4  Accelerate analysis and discovery of cancer bi...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://aws.amazon.com/blogs/machine-learning/...   \n",
       "1  https://aws.amazon.com/blogs/machine-learning/...   \n",
       "2  https://aws.amazon.com/blogs/machine-learning/...   \n",
       "3  https://aws.amazon.com/blogs/machine-learning/...   \n",
       "4  https://aws.amazon.com/blogs/machine-learning/...   \n",
       "\n",
       "                                description_original               source  \\\n",
       "0  Ensemble models are becoming popular within th...  AWS-MachineLearning   \n",
       "1  In this post, we explore a solution for implem...  AWS-MachineLearning   \n",
       "2  In this post, we explore Clearwater Analytics’...  AWS-MachineLearning   \n",
       "3  In this post, we demonstrate how we innovated ...  AWS-MachineLearning   \n",
       "4  Bedrock multi-agent collaboration enables deve...  AWS-MachineLearning   \n",
       "\n",
       "                published_at               folder  \n",
       "0 2024-12-16 17:28:15.600928  AWS-MachineLearning  \n",
       "1 2024-12-16 17:28:15.601269  AWS-MachineLearning  \n",
       "2 2024-12-16 17:28:15.601499  AWS-MachineLearning  \n",
       "3 2024-12-16 17:28:15.601714  AWS-MachineLearning  \n",
       "4 2024-12-16 17:28:15.601947  AWS-MachineLearning  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(content_list)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919d5aa-3437-45f0-b3e4-40e12c3f81a6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 通过BeautifulSoup爬取deeplearning_ai内容\n",
    "用于解析和处理 HTML/XML 的 Python 库，常与 lxml 或 html.parser 等解析器一起使用。它提供了一套简洁的 API，方便开发者从网页或 XML 数据中提取结构化信息。能解析任何 HTML/XML 内容，不依赖于特定格式（如 RSS/Atom）。可以处理不完整或格式不规范的 HTML。\n",
    "\n",
    "![bs4爬取网页内容](pictures/bs4爬取网页内容.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2dc53f92-0a43-46a9-a181-452288bc2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplearning_ai(source_content):\n",
    "    content_list = []\n",
    "    # 获取网页内容并解析文章\n",
    "    notice_items = BeautifulSoup(requests.get(source_content.get('url')).text, 'lxml').find_all('article')\n",
    "    for item in notice_items:\n",
    "        target_link = ''\n",
    "        try:\n",
    "            # 提取文章链接\n",
    "            target_link = next((source_content.get('domain') + link.get('href') for link in item.find_all('a') if 'issue' in link.get('href')), '')\n",
    "            # 提取发布时间和简介\n",
    "            divs = item.find_all('div')\n",
    "            published_at = divs[1].find_all('div')[0].text if len(divs) >= 2 else ''\n",
    "            description_original = divs[1].find_all('div')[1].text if len(divs) >= 2 else ''\n",
    "            # 构建文章字典\n",
    "            content_dict = {\n",
    "                'title': BeautifulSoup(item.find('h2').text.strip(), \"lxml\").get_text(),\n",
    "                'link': target_link,\n",
    "                'description_original': BeautifulSoup(description_original, \"lxml\").get_text(),\n",
    "                'source': source_content.get('folder'),\n",
    "                'published_at': convert_to_cst(BeautifulSoup(published_at, \"lxml\").get_text()),\n",
    "                'folder': source_content.get('folder')\n",
    "            }\n",
    "            content_list.append(content_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ca5ceac3-65fe-4e58-b353-c08814c407f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_496330/4074190371.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  'title': BeautifulSoup(item.find('h2').text.strip(), \"lxml\").get_text(),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>description_original</th>\n",
       "      <th>source</th>\n",
       "      <th>published_at</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon Nova’s Competitive Price/Performance, O...</td>\n",
       "      <td>https://www.deeplearning.ai/the-batch/issue-279/</td>\n",
       "      <td>The Batch AI News and Insights: AI Product Man...</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "      <td>2024-12-17 13:02:00.363918</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI Agents Spend Real Money, Breaking Jailbreak...</td>\n",
       "      <td>https://www.deeplearning.ai/the-batch/issue-278/</td>\n",
       "      <td>The Batch AI News and Insights: AI Agents Spen...</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "      <td>2024-12-17 13:02:00.364810</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSeek Takes On OpenAI, Robots Fold Laundry,...</td>\n",
       "      <td>https://www.deeplearning.ai/the-batch/issue-277/</td>\n",
       "      <td>The Batch AI News and Insights: DeepSeek Takes...</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "      <td>2024-12-17 13:02:00.365564</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next-Gen Models Show Limited Gains, Real-Time ...</td>\n",
       "      <td>https://www.deeplearning.ai/the-batch/issue-276/</td>\n",
       "      <td>The Batch AI News and Insights: A small number...</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "      <td>2024-12-17 13:02:00.366284</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama On the Battlefield, Mixture of Experts P...</td>\n",
       "      <td>https://www.deeplearning.ai/the-batch/issue-275/</td>\n",
       "      <td>The Batch AI News and Insights: Large language...</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "      <td>2024-12-17 13:02:00.367147</td>\n",
       "      <td>deeplearning_ai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Amazon Nova’s Competitive Price/Performance, O...   \n",
       "1  AI Agents Spend Real Money, Breaking Jailbreak...   \n",
       "2  DeepSeek Takes On OpenAI, Robots Fold Laundry,...   \n",
       "3  Next-Gen Models Show Limited Gains, Real-Time ...   \n",
       "4  Llama On the Battlefield, Mixture of Experts P...   \n",
       "\n",
       "                                               link  \\\n",
       "0  https://www.deeplearning.ai/the-batch/issue-279/   \n",
       "1  https://www.deeplearning.ai/the-batch/issue-278/   \n",
       "2  https://www.deeplearning.ai/the-batch/issue-277/   \n",
       "3  https://www.deeplearning.ai/the-batch/issue-276/   \n",
       "4  https://www.deeplearning.ai/the-batch/issue-275/   \n",
       "\n",
       "                                description_original           source  \\\n",
       "0  The Batch AI News and Insights: AI Product Man...  deeplearning_ai   \n",
       "1  The Batch AI News and Insights: AI Agents Spen...  deeplearning_ai   \n",
       "2  The Batch AI News and Insights: DeepSeek Takes...  deeplearning_ai   \n",
       "3  The Batch AI News and Insights: A small number...  deeplearning_ai   \n",
       "4  The Batch AI News and Insights: Large language...  deeplearning_ai   \n",
       "\n",
       "                published_at           folder  \n",
       "0 2024-12-17 13:02:00.363918  deeplearning_ai  \n",
       "1 2024-12-17 13:02:00.364810  deeplearning_ai  \n",
       "2 2024-12-17 13:02:00.365564  deeplearning_ai  \n",
       "3 2024-12-17 13:02:00.366284  deeplearning_ai  \n",
       "4 2024-12-17 13:02:00.367147  deeplearning_ai  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "content_web_list = deeplearning_ai(web_source_list[1])\n",
    "display(pd.DataFrame(content_web_list)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b5683-d803-42fc-8e32-ca4a6e54a236",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.3 通过selenium模拟浏览器爬取OpenAI News\n",
    "可以模拟用户操作浏览器的行为，它适用于网页测试、爬虫开发、表单提交和动态内容抓取等场景。无需打开实际浏览器窗口（Headless Mode），更适合后台运行任务。模拟点击、输入、拖拽、滚动等用户行为，支持执行自定义的 JavaScript 脚本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714dacfc-e7f4-4bad-9c8b-c456bbb80dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lxml\n",
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7fb3e-8913-423d-9be5-72ff50d13774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenAI_News(self):\n",
    "    # 设置Chrome浏览器选项\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "\n",
    "    # 设置无头模式\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # 隐藏自动化特征\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    chrome_options.add_argument(\"--disable-web-security\")  # 允许跨域\n",
    "    chrome_options.add_argument(\"--allow-running-insecure-content\")\n",
    "    chrome_options.add_argument(\"--disable-site-isolation-trials\")\n",
    "\n",
    "    # 修改 User-Agent\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "    # 初始化浏览器\n",
    "    prefs = {\n",
    "        \"profile.managed_default_content_settings.images\": 2,\n",
    "        \"profile.default_content_setting_values.notifications\": 2,\n",
    "        \"profile.default_content_setting_values.stylesheets\": 2,\n",
    "        \"profile.default_content_setting_values.cookies\": 2,\n",
    "        \"profile.default_content_setting_values.javascript\": 1,\n",
    "        \"profile.default_content_setting_values.plugins\": 2,\n",
    "        \"profile.default_content_setting_values.popups\": 2,\n",
    "        \"profile.default_content_setting_values.geolocation\": 2,\n",
    "        \"profile.default_content_setting_values.media_stream\": 2,\n",
    "    }\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    # 初始化浏览器\n",
    "    service = Service(ChromeDriverManager().install(), log_path='/dev/null')\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    # 打开网页\n",
    "    driver.get(self.source_url)\n",
    "    # 等待页面加载完成（根据需要调整时间）\n",
    "    time.sleep(30)\n",
    "    # 获取渲染后的页面源代码\n",
    "    html_content = driver.page_source\n",
    "    # 关闭浏览器\n",
    "    driver.quit()\n",
    "\n",
    "    # 解析HTML内容\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "    # 查找符合条件的<div>\n",
    "    notice_items = soup.select('div.col-span-1.transition.opacity-1.ease-curve-c.duration-400')\n",
    "    for item in notice_items:\n",
    "        try:\n",
    "            # 获取link\n",
    "            other_content = item.find_all('span')\n",
    "            content = item.find('a')\n",
    "            try:\n",
    "                published_at = other_content[1].text\n",
    "            except Exception as e:\n",
    "                logging.error(f\"文章“published_at”字段解析失败:{e}，文章内容为\\n{item}\")\n",
    "                published_at = ''\n",
    "            try:\n",
    "                description_original = other_content[0].text\n",
    "            except Exception as e:\n",
    "                logging.info(f\"文章“description_original”字段解析失败:{e}，文章内容为\\n{item}\")\n",
    "                description_original = ''\n",
    "            if content:\n",
    "                content_dict = {\n",
    "                    'title': BeautifulSoup(content.get('aria-label'), \"lxml\").get_text(),\n",
    "                    'link': self.domain + content.get('href'),\n",
    "                    'description_original': BeautifulSoup(description_original, \"lxml\").get_text(),\n",
    "                    'source': self.source,\n",
    "                    'published_at': convert_to_cst(BeautifulSoup(published_at, \"lxml\").get_text()),\n",
    "                    'folder': self.folder\n",
    "                }\n",
    "                self.article_data_list.append(content_dict)\n",
    "            else:\n",
    "                logging.info(\"未找到元素\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"解析文章失败:{item} {e}，文章内容为\\n{item}\")\n",
    "            continue\n",
    "    return self.article_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0212c0-4e74-456c-9f67-dadeff2417a3",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "## 三、数据库插入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f919207a-ed9e-4291-8a62-dee54ec11921",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_message = config['mysql']\n",
    "connection = mysql.connector.connect(\n",
    "        host=mysql_message['host'],\n",
    "        port=mysql_message['port'],\n",
    "        database=mysql_message['database'],\n",
    "        user=mysql_message['user'],\n",
    "        password=mysql_message['password'],\n",
    "        ssl_disabled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e328b915-2d0a-4ca3-b4f8-ac584b052942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Skipping】 Article 'How Amazon trains sequential ensemble models at scale with Amazon SageMaker Pipelines' already exists, skipping.\n",
      "【Skipping】 Article 'Implementing login node load balancing in SageMaker HyperPod for enhanced multi-user experience' already exists, skipping.\n",
      "【Skipping】 Article 'How Clearwater Analytics is revolutionizing investment management with generative AI and Amazon SageMaker JumpStart' already exists, skipping.\n",
      "【Skipping】 Article 'How Twitch used agentic workflow with RAG on Amazon Bedrock to supercharge ad sales' already exists, skipping.\n",
      "【Skipping】 Article 'Accelerate analysis and discovery of cancer biomarkers with Amazon Bedrock Agents' already exists, skipping.\n",
      "【Skipping】 Article 'Accelerate your ML lifecycle using the new and improved Amazon SageMaker Python SDK – Part 2: ModelBuilder' already exists, skipping.\n",
      "【Skipping】 Article 'Accelerate your ML lifecycle using the new and improved Amazon SageMaker Python SDK – Part 1: ModelTrainer' already exists, skipping.\n",
      "【Skipping】 Article 'Amazon Q Apps supports customization and governance of generative AI-powered apps' already exists, skipping.\n",
      "【Skipping】 Article 'Answer questions from tables embedded in documents with Amazon Q Business' already exists, skipping.\n",
      "【Skipping】 Article 'How AWS sales uses Amazon Q Business for customer engagement' already exists, skipping.\n",
      "【Skipping】 Article 'Discover insights from your Amazon Aurora PostgreSQL database using the Amazon Q Business connector' already exists, skipping.\n",
      "【Skipping】 Article 'How Tealium built a chatbot evaluation platform with Ragas and Auto-Instruct using AWS generative AI services' already exists, skipping.\n",
      "【Skipping】 Article 'EBSCOlearning scales assessment generation for their online learning content with generative AI' already exists, skipping.\n",
      "【Skipping】 Article 'Pixtral 12B is now available on Amazon SageMaker JumpStart' already exists, skipping.\n",
      "【Skipping】 Article 'Talk to your slide deck using multimodal foundation models on Amazon Bedrock – Part 3' already exists, skipping.\n",
      "【Skipping】 Article 'Automate actions across enterprise applications using Amazon Q Business plugins' already exists, skipping.\n",
      "【Skipping】 Article 'Accelerating ML experimentation with enhanced security: AWS PrivateLink support for Amazon SageMaker with MLflow' already exists, skipping.\n",
      "【Skipping】 Article 'Mistral-NeMo-Instruct-2407 and Mistral-NeMo-Base-2407 are now available on SageMaker JumpStart' already exists, skipping.\n",
      "【Skipping】 Article 'Advancing AI trust with new responsible AI tools, capabilities, and resources' already exists, skipping.\n",
      "【Skipping】 Article 'Deploy RAG applications on Amazon SageMaker JumpStart using FAISS' already exists, skipping.\n"
     ]
    }
   ],
   "source": [
    "if connection is not None:\n",
    "    cursor = connection.cursor()\n",
    "    for content in content_list:\n",
    "        try:\n",
    "            # 检查是否存在相同title的文章\n",
    "            check_query = \"SELECT 1 FROM original_article_table WHERE title = %s\"\n",
    "            cursor.execute(check_query, (content['title'],))\n",
    "            if cursor.fetchone():\n",
    "                # 如果找到了，说明已存在相同title的文章，跳过插入\n",
    "                print(f\"【Skipping】 Article '{content['title']}' already exists, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 构造SQL插入语句，包括所有字段\n",
    "            insert_query = \"\"\"\n",
    "                        INSERT INTO original_article_table (title, link, description_original, source, published_at, is_processed, folder) \n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                        ON DUPLICATE KEY UPDATE title=VALUES(title)\n",
    "                        \"\"\"\n",
    "            # 从article_content字典中提取字段值\n",
    "            title = content['title']\n",
    "            link = content['link']\n",
    "            description_original = content['description_original']\n",
    "            folder = content['folder']\n",
    "            source = content['source']  # 假设source字段从类属性获取或其他逻辑\n",
    "            published_at = content['published_at']  # 确保published_at是datetime对象\n",
    "            is_processed = False\n",
    "\n",
    "            # 执行SQL语句\n",
    "            cursor.execute(insert_query, (title, link, description_original, source, published_at, is_processed, folder))\n",
    "            connection.commit()\n",
    "            print(f\"【successfully】 Article '{title}' inserted successfully.\")\n",
    "        except mysql.connector.Error as e:\n",
    "            print(f\"【Failed】 Failed to insert article '{content['title']}'. MySQL Error: {e}\")\n",
    "            continue  # 即使发生错误也继续尝试插入下一篇文章\n",
    "    cursor.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b68b1c1f-0dca-4ef2-9153-0bc8419a6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
